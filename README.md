## toxic_comment detection using Kaggle Dataset

In this project, we compare the performance of machine learning and deep learning mod- els, including Naive Bayes, Logistic Regres- sion, Decision Tree, LSTM, CNN, and BERT on a toxic comment detection challenge from Kaggle. The main goal of this project is to help mitigate content moderation. Therefore, After training the models, we analyze popular online and social media platforms, including Reddit and Twitter, by detecting toxic content using the trained model. We further show the vulnerability of the models using adversarial examples.


### Scripts
NOTE: if you have trouble loading the scripts, click reload after the first fail. 

#### Training
- [script: non_nn_models.ipynb](https://github.com/dchen236/toxic_comment/blob/master/non_nn_models.ipynb) for trainning non neural networks including Naive Bayes, Logistic Regression and Decision Tree
- [script: BERT_Toxic.ipynb](https://github.com/dchen236/toxic_comment/blob/master/BERT_Toxic.ipynb) for trainning BERT
- [script: kaggle_benchmark.py](https://github.com/dchen236/toxic_comment/blob/master/kaggle_benchmark.py) for trainning CNN
- [script: tBERT_tSNE.ipynb](https://github.com/dchen236/toxic_comment/blob/master/BERT_tSNE.ipynb) for t-SNE visualization
TODO: add LSTM, CNN + BERT , t-SNE

#### Social / Online Platform Analysis
- [covid19-tweet-eda.ipynb](https://github.com/dchen236/toxic_comment/blob/master/covid19-tweet-eda.ipynb) covid19 related tweets EDA

#### Adversarial Vulnerability Attack
- [script](https://github.com/dchen236/toxic_comment/blob/master/Perspective_API.ipynb) of adversarial attack with Perspective API.

Team Member: 
- Danni Chen
- Fuzail Khan
- Don Le

